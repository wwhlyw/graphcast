60
60
----------------init---------------------
-------------start training-------------
--------------train phase1--------------
--------------train phase2--------------
	 epoch: 0|iter: 99: train_loss: 7228.967005661099
	 epoch: 0|iter: 199: train_loss: 3039.8228412488365
	 epoch: 0|iter: 299: train_loss: 1027.245359078426
	 epoch: 0|iter: 399: train_loss: 3866.2227892318006
	 epoch: 0|iter: 499: train_loss: 3455.3050417164577
	 epoch: 0|iter: 599: train_loss: 420.9027156079846
	 epoch: 0|iter: 699: train_loss: 2052.529204639591
	 epoch: 0|iter: 799: train_loss: 2129.3886363660095
	 epoch: 0|iter: 899: train_loss: 2415.772542343509
	 epoch: 0|iter: 999: train_loss: 67.98268207834478
	 epoch: 0|iter: 1099: train_loss: 572.9142127246721
	 epoch: 0|iter: 1199: train_loss: 570.8689726570349
	 epoch: 0|iter: 1299: train_loss: 884.3616158640301
	 epoch: 0|iter: 1399: train_loss: 337.82716828119345
	 epoch: 0|iter: 1499: train_loss: 899.7029784463105
	 epoch: 0|iter: 1599: train_loss: 1767.5538614692175
	 epoch: 0|iter: 1699: train_loss: 394.4753265801598
	 epoch: 0|iter: 1799: train_loss: 228.34375428889712
	 epoch: 0|iter: 1899: train_loss: 2438.79691106597
	 epoch: 0|iter: 1999: train_loss: 735.8176122650679
	 epoch: 0|iter: 2099: train_loss: 787.4320053851354
	 epoch: 0|iter: 2199: train_loss: 121.35176523883055
	 epoch: 0|iter: 2299: train_loss: 2169.401156591754
	 epoch: 0|iter: 2399: train_loss: 807.6443767028978
	 epoch: 0|iter: 2499: train_loss: 155.6461411611811
	 epoch: 0|iter: 2599: train_loss: 662.6997645540555
	 epoch: 0|iter: 2699: train_loss: 1720.4314836130704
	 epoch: 0|iter: 2799: train_loss: 3963.988706189567
	 epoch: 0|iter: 2899: train_loss: 741.991308311053
	 epoch: 0|iter: 2999: train_loss: 100.6846868953751
	 epoch: 0|iter: 3099: train_loss: 110.61944509247698
	 epoch: 0|iter: 3199: train_loss: 4454.145023354688
	 epoch: 0|iter: 3299: train_loss: 166.230635815935
	 epoch: 0|iter: 3399: train_loss: 1570.318072686316
	 epoch: 0|iter: 3499: train_loss: 79.53935002606904
	 epoch: 0|iter: 3599: train_loss: 243.7875776698096
	 epoch: 0|iter: 3699: train_loss: 224.56126155737348
	 epoch: 0|iter: 3799: train_loss: 236.5804311757949
	 epoch: 0|iter: 3899: train_loss: 85.51876232415279
	 epoch: 0|iter: 3999: train_loss: 83.1313429433694
	 epoch: 0|iter: 4099: train_loss: 399.87375521883666
	 epoch: 0|iter: 4199: train_loss: 103.78905765789794
	 epoch: 0|iter: 4299: train_loss: 171.66665161129444
	 epoch: 0|iter: 4399: train_loss: 448.7989735098175
	 epoch: 0|iter: 4499: train_loss: 90.75720963720649
	 epoch: 0|iter: 4599: train_loss: 294.4436893715907
	 epoch: 0|iter: 4699: train_loss: 75.74676052907057
	 epoch: 0|iter: 4799: train_loss: 70.1930790407743
	 epoch: 0|iter: 4899: train_loss: 210.56945612190646
	 epoch: 0|iter: 4999: train_loss: 248.77971694147672
	 epoch: 0|iter: 5099: train_loss: 266.57943650745756
	 epoch: 0|iter: 5199: train_loss: 121.98996197640999
	 epoch: 0|iter: 5299: train_loss: 346.89679661720487
	 epoch: 0|iter: 5399: train_loss: 122.69356770840221
	 epoch: 0|iter: 5499: train_loss: 159.5034816771971
	 epoch: 0|iter: 5599: train_loss: 291.4266500643934
	 epoch: 0|iter: 5699: train_loss: 67.15897664669328
	 epoch: 0|iter: 5799: train_loss: 88.34739755310454
	 epoch: 0|iter: 5899: train_loss: 101.64702136500733
	 epoch: 0|iter: 5999: train_loss: 148.4004387667823
	 epoch: 0|iter: 6099: train_loss: 295.4747912803623
	 epoch: 0|iter: 6199: train_loss: 623.7480006323395
	 epoch: 0|iter: 6299: train_loss: 58.565906857032715
	 epoch: 0|iter: 6399: train_loss: 93.65611555027573
	 epoch: 0|iter: 6499: train_loss: 47.73122945694793
	 epoch: 0|iter: 6599: train_loss: 6622.6728830974025
	 epoch: 0|iter: 6699: train_loss: 159.3221227236621
	 epoch: 0|iter: 6799: train_loss: 136.18263347334056
	 epoch: 0|iter: 6899: train_loss: 53.16740703379358
	 epoch: 0|iter: 6999: train_loss: 140.79933590623338
	 epoch: 0|iter: 7099: train_loss: 634.5905050314752
	 epoch: 0|iter: 7199: train_loss: 253.0994168124301
	 epoch: 0|iter: 7299: train_loss: 89.21198463682492
	 epoch: 0|iter: 7399: train_loss: 213.02312822164623
	 epoch: 0|iter: 7499: train_loss: 41.8951672829857
	 epoch: 0|iter: 7599: train_loss: 144.60006925241728
	 epoch: 0|iter: 7699: train_loss: 2052.613056007277
	 epoch: 0|iter: 7799: train_loss: 67.21743700273522
	 epoch: 0|iter: 7899: train_loss: 39.310670624031694
	 epoch: 0|iter: 7999: train_loss: 79.96119720066424
	 epoch: 0|iter: 8099: train_loss: 270.39895643994834
	 epoch: 0|iter: 8199: train_loss: 151.81998174965838
	 epoch: 0|iter: 8299: train_loss: 373.01609666184515
	 epoch: 0|iter: 8399: train_loss: 103.38790522112103
	 epoch: 0|iter: 8499: train_loss: 86.09812937043401
	 epoch: 0|iter: 8599: train_loss: 173.5399228839833
	 epoch: 0|iter: 8699: train_loss: 184.01912513353716
	 epoch: 0|iter: 8799: train_loss: 74.66713454079077
	 epoch: 0|iter: 8899: train_loss: 91.00784142000583
	 epoch: 0|iter: 8999: train_loss: 67.08318298957509
	 epoch: 0|iter: 9099: train_loss: 133.60684643084417
	 epoch: 0|iter: 9199: train_loss: 250.16124413486665
	 epoch: 0|iter: 9299: train_loss: 745.0704806432138
	 epoch: 0|iter: 9399: train_loss: 141.94681754711206
	 epoch: 0|iter: 9499: train_loss: 1282.2314455896278
	 epoch: 0|iter: 9599: train_loss: 1673.0280030416693
	 epoch: 0|iter: 9699: train_loss: 614.8227558585908
	 epoch: 0|iter: 9799: train_loss: 73.1508028579526
	 epoch: 0|iter: 9899: train_loss: 551.899380419292
	 epoch: 0|iter: 9999: train_loss: 481.2081977071166
	 epoch: 0|iter: 10099: train_loss: 26.057103643602073
	 epoch: 0|iter: 10199: train_loss: 77.17749378821237
	 epoch: 0|iter: 10299: train_loss: 26.163953342943447
	 epoch: 0|iter: 10399: train_loss: 172.26421993166124
	 epoch: 0|iter: 10499: train_loss: 99.18117416806597
	 epoch: 0|iter: 10599: train_loss: 113.45877676161197
	 epoch: 0|iter: 10699: train_loss: 251.16557367215697
	 epoch: 0|iter: 10799: train_loss: 50.00139122689928
	 epoch: 0|iter: 10899: train_loss: 459.6909615281657
	 epoch: 0|iter: 10999: train_loss: 100.85613479355686
	 epoch: 0|iter: 11099: train_loss: 534.2419482963846
	 epoch: 0|iter: 11199: train_loss: 371.9742593386494
	 epoch: 0|iter: 11299: train_loss: 91.33905681348759
	 epoch: 0|iter: 11399: train_loss: 193.8634791520313
	 epoch: 0|iter: 11499: train_loss: 91.51806752802548
	 epoch: 0|iter: 11599: train_loss: 25.021520146289458
	 epoch: 0|iter: 11699: train_loss: 90.20367527497646
	 epoch: 0|iter: 11799: train_loss: 43.67686640545808
	 epoch: 0|iter: 11899: train_loss: 38.20523996763115
	 epoch: 0|iter: 11999: train_loss: 99.9453533252806
	 epoch: 0|iter: 12099: train_loss: 150.66104566092514
	 epoch: 0|iter: 12199: train_loss: 202.6225399172461
	 epoch: 0|iter: 12299: train_loss: 100.86599907210324
	 epoch: 0|iter: 12399: train_loss: 306.6910966757376
	 epoch: 0|iter: 12499: train_loss: 207.08014948046252
	 epoch: 0|iter: 12599: train_loss: 131.95542696133634
	 epoch: 0|iter: 12699: train_loss: 734.7498198412399
	 epoch: 0|iter: 12799: train_loss: 141.38488273821795
	 epoch: 0|iter: 12899: train_loss: 43.24840927967731
	 epoch: 0|iter: 12999: train_loss: 302.21728301169065
	 epoch: 0|iter: 13099: train_loss: 98.05294723452609
	 epoch: 0|iter: 13199: train_loss: 96.24984678692573
	 epoch: 0|iter: 13299: train_loss: 176.41142883706667
	 epoch: 0|iter: 13399: train_loss: 33.421661636358515
	 epoch: 0|iter: 13499: train_loss: 142.97341963884054
	 epoch: 0|iter: 13599: train_loss: 183.4637776033278
	 epoch: 0|iter: 13699: train_loss: 33.15719090634463
	 epoch: 0|iter: 13799: train_loss: 85.02445454921275
	 epoch: 0|iter: 13899: train_loss: 67.86604099099185
	 epoch: 0|iter: 13999: train_loss: 81.4988612855047
	 epoch: 0|iter: 14099: train_loss: 169.6147899457747
	 epoch: 0|iter: 14199: train_loss: 118.21189294756726
	 epoch: 0|iter: 14299: train_loss: 58.01274641767329
	 epoch: 0|iter: 14399: train_loss: 358.9958063838364
	 epoch: 0|iter: 14499: train_loss: 156.0003340719049
	 epoch: 0|iter: 14599: train_loss: 42.656070521052236
	 epoch: 0|iter: 14699: train_loss: 47.51206301601537
	 epoch: 0|iter: 14799: train_loss: 137.03164678631907
	 epoch: 0|iter: 14899: train_loss: 58.429281769861234
	 epoch: 0|iter: 14999: train_loss: 97.62046730052576
	 epoch: 0|iter: 15099: train_loss: 251.20384413474474
	 epoch: 0|iter: 15199: train_loss: 32.664735386733675
	 epoch: 0|iter: 15299: train_loss: 32.80867922132898
	 epoch: 0|iter: 15399: train_loss: 187.00221531330544
	 epoch: 0|iter: 15499: train_loss: 597.2369680963719
	 epoch: 0|iter: 15599: train_loss: 41.07702051806137
	 epoch: 0|iter: 15699: train_loss: 368.9197281088757
	 epoch: 0|iter: 15799: train_loss: 139.74085385980695
	 epoch: 0|iter: 15899: train_loss: 85.69671727190041
	 epoch: 0|iter: 15999: train_loss: 50.32773251220965
	 epoch: 0|iter: 16099: train_loss: 395.00074819465283
	 epoch: 0|iter: 16199: train_loss: 146.096350134785
	 epoch: 0|iter: 16299: train_loss: 60.949215668309556
	 epoch: 0|iter: 16399: train_loss: 32.01335008211026
	 epoch: 0|iter: 16499: train_loss: 75.92521259101278
	 epoch: 0|iter: 16599: train_loss: 155.7519678230273
	 epoch: 0|iter: 16699: train_loss: 87.11997248231779
	 epoch: 0|iter: 16799: train_loss: 57.52975908879779
	 epoch: 0|iter: 16899: train_loss: 184.48894358358973
	 epoch: 0|iter: 16999: train_loss: 175.28466661685684
	 epoch: 0|iter: 17099: train_loss: 465.18075742364334
	 epoch: 0|iter: 17199: train_loss: 49.27531738699791
	 epoch: 0|iter: 17299: train_loss: 72.87429165960826
	 epoch: 0|iter: 17399: train_loss: 116.12736440178948
	 epoch: 0|iter: 17499: train_loss: 89.68137885954395
	 epoch: 0|iter: 17599: train_loss: 60.560700220137036
	 epoch: 0|iter: 17699: train_loss: 2937.391973605023
	 epoch: 0|iter: 17799: train_loss: 157.10781291739357
	 epoch: 0|iter: 17899: train_loss: 81.54633516422997
	 epoch: 0|iter: 17999: train_loss: 985.7349736084756
	 epoch: 0|iter: 18099: train_loss: 52.60785109744234
	 epoch: 0|iter: 18199: train_loss: 91.23865660197923
	 epoch: 0|iter: 18299: train_loss: 206.82913277779076
	 epoch: 0|iter: 18399: train_loss: 82.9061302522966
	 epoch: 0|iter: 18499: train_loss: 33.26466154052859
	 epoch: 0|iter: 18599: train_loss: 27.431741446526846
	 epoch: 0|iter: 18699: train_loss: 529.4268906968348
	 epoch: 0|iter: 18799: train_loss: 140.30172669531862
	 epoch: 0|iter: 18899: train_loss: 74.22072371553695
	 epoch: 0|iter: 18999: train_loss: 176.56718659454575
	 epoch: 0|iter: 19099: train_loss: 61.59515186365706
	 epoch: 0|iter: 19199: train_loss: 25.55784727008623
	 epoch: 0|iter: 19299: train_loss: 121.69461780957235
	 epoch: 0|iter: 19399: train_loss: 253.12568295827
	 epoch: 0|iter: 19499: train_loss: 38.448838755529046
	 epoch: 0|iter: 19599: train_loss: 104.2149170668806
	 epoch: 0|iter: 19699: train_loss: 50.18861220400692
	 epoch: 0|iter: 19799: train_loss: 233.42859513687407
	 epoch: 0|iter: 19899: train_loss: 86.38434883280662
	 epoch: 0|iter: 19999: train_loss: 1885.270564513616
	 epoch: 0|iter: 20099: train_loss: 104.74701521841656
	 epoch: 0|iter: 20199: train_loss: 190.19349379277674
	 epoch: 0|iter: 20299: train_loss: 383.22926931761305
	 epoch: 0|iter: 20399: train_loss: 139.96389810233586
	 epoch: 0|iter: 20499: train_loss: 19.81099640047991
	 epoch: 0|iter: 20599: train_loss: 141.18593922188288
	 epoch: 0|iter: 20699: train_loss: 738.1881175079652
	 epoch: 0|iter: 20799: train_loss: 1068.3211432778644
	 epoch: 0|iter: 20899: train_loss: 160.08079207169817
	 epoch: 0|iter: 20999: train_loss: 130.73020247465436
	 epoch: 0|iter: 21099: train_loss: 78.61334173202518
	 epoch: 0|iter: 21199: train_loss: 83.24542175077876
	 epoch: 0|iter: 21299: train_loss: 65.46435602097326
	 epoch: 0|iter: 21399: train_loss: 52.45361654330132
	 epoch: 0|iter: 21499: train_loss: 17.388478776453677
	 epoch: 0|iter: 21599: train_loss: 39.530314446357075
	 epoch: 0|iter: 21699: train_loss: 313.47578901865484
	 epoch: 0|iter: 21799: train_loss: 43.694584138606444
	 epoch: 0|iter: 21899: train_loss: 147.37566145940252
	 epoch: 0|iter: 21999: train_loss: 50.4787322818254
	 epoch: 0|iter: 22099: train_loss: 173.7146486877023
	 epoch: 0|iter: 22199: train_loss: 395.2645686495156
	 epoch: 0|iter: 22299: train_loss: 122.95390540791534
	 epoch: 0|iter: 22399: train_loss: 1364.1193177836747
	 epoch: 0|iter: 22499: train_loss: 27.563877095896753
	 epoch: 0|iter: 22599: train_loss: 46.75289912930316
	 epoch: 0|iter: 22699: train_loss: 164.99971127458807
	 epoch: 0|iter: 22799: train_loss: 142.82809409412744
	 epoch: 0|iter: 22899: train_loss: 310.9189061462117
	 epoch: 0|iter: 22999: train_loss: 54.68895667123084
	 epoch: 0|iter: 23099: train_loss: 196.2198620401874
	 epoch: 0|iter: 23199: train_loss: 40.57266533814473
	 epoch: 0|iter: 23299: train_loss: 333.4364212962291
	 epoch: 0|iter: 23399: train_loss: 211.68639751709324
	 epoch: 0|iter: 23499: train_loss: 38.12376620525723
	 epoch: 0|iter: 23599: train_loss: 235.62639962993876
	 epoch: 0|iter: 23699: train_loss: 56.307220902992555
	 epoch: 0|iter: 23799: train_loss: 44.60806005264286
	 epoch: 0|iter: 23899: train_loss: 61.26200786713496
	 epoch: 0|iter: 23999: train_loss: 532.5431138075378
	 epoch: 0|iter: 24099: train_loss: 64.06293331040457
	 epoch: 0|iter: 24199: train_loss: 131.96113267886687
	 epoch: 0|iter: 24299: train_loss: 24.798843432992435
	 epoch: 0|iter: 24399: train_loss: 15.535739723675405
	 epoch: 0|iter: 24499: train_loss: 1164.6304312001578
	 epoch: 0|iter: 24599: train_loss: 70.05440142020255
	 epoch: 0|iter: 24699: train_loss: 34.64222973715406
	 epoch: 0|iter: 24799: train_loss: 21.141275410185056
	 epoch: 0|iter: 24899: train_loss: 787.9753612903719
	 epoch: 0|iter: 24999: train_loss: 57.64799201076405
	 epoch: 0|iter: 25099: train_loss: 467.7814683878434
	 epoch: 0|iter: 25199: train_loss: 41.78619124601553
	 epoch: 0|iter: 25299: train_loss: 115.26551843693294
	 epoch: 0|iter: 25399: train_loss: 231.97274440525842
	 epoch: 0|iter: 25499: train_loss: 79.89341592861273
	 epoch: 0|iter: 25599: train_loss: 3121.817551329747
	 epoch: 0|iter: 25699: train_loss: 60.082215889162256
	 epoch: 0|iter: 25799: train_loss: 25.61688359349966
	 epoch: 0|iter: 25899: train_loss: 101.00969138486826
	 epoch: 0|iter: 25999: train_loss: 225.4201213274363
	 epoch: 0|iter: 26099: train_loss: 127.37667995839371
	 epoch: 0|iter: 26199: train_loss: 63.800992996753294
	 epoch: 0|iter: 26299: train_loss: 29.543775441015427
	 epoch: 0|iter: 26399: train_loss: 47.639731309528926
	 epoch: 0|iter: 26499: train_loss: 239.7278260389274
	 epoch: 0|iter: 26599: train_loss: 72.43134289532098
	 epoch: 0|iter: 26699: train_loss: 112.6807433358195
	 epoch: 0|iter: 26799: train_loss: 19.120017038005976
	 epoch: 0|iter: 26899: train_loss: 43.10953961126976
	 epoch: 0|iter: 26999: train_loss: 99.20283456409871
	 epoch: 0|iter: 27099: train_loss: 41.58493144118723
	 epoch: 0|iter: 27199: train_loss: 32.89779809120437
	 epoch: 0|iter: 27299: train_loss: 71.14404316365537
	 epoch: 0|iter: 27399: train_loss: 58.45639612903761
	 epoch: 0|iter: 27499: train_loss: 438.0636524326921
	 epoch: 0|iter: 27599: train_loss: 89.51832231370464
	 epoch: 0|iter: 27699: train_loss: 238.8052113522719
	 epoch: 0|iter: 27799: train_loss: 154.30700576209233
	 epoch: 0|iter: 27899: train_loss: 21.5885744438333
	 epoch: 0|iter: 27999: train_loss: 18.473816937955274
	 epoch: 0|iter: 28099: train_loss: 27.88524603514141
	 epoch: 0|iter: 28199: train_loss: 86.40340051935118
	 epoch: 0|iter: 28299: train_loss: 92.25104174457806
	 epoch: 0|iter: 28399: train_loss: 128.44390364589614
	 epoch: 0|iter: 28499: train_loss: 105.93458305947357
	 epoch: 0|iter: 28599: train_loss: 172.99504886089622
	 epoch: 0|iter: 28699: train_loss: 69.51441378254498
	 epoch: 0|iter: 28799: train_loss: 61.59112539982527
	 epoch: 0|iter: 28899: train_loss: 75.91896084528187
	 epoch: 0|iter: 28999: train_loss: 40.49747506277811
	 epoch: 0|iter: 29099: train_loss: 33.96496500798944
	 epoch: 0|iter: 29199: train_loss: 25.724960277770997
	 epoch: 0|iter: 29299: train_loss: 125.28875652904067
	 epoch: 0|iter: 29399: train_loss: 42.03205660210291
	 epoch: 0|iter: 29499: train_loss: 13.720822060118891
	 epoch: 0|iter: 29599: train_loss: 37.757733274333205
	 epoch: 0|iter: 29699: train_loss: 46.790551954
	 epoch: 0|iter: 29799: train_loss: 91.95440735321625
	 epoch: 0|iter: 29899: train_loss: 89.5136007633239
	 epoch: 0|iter: 29999: train_loss: 67.98980753768141
	 epoch: 0|iter: 30099: train_loss: 204.60020709099615
	 epoch: 0|iter: 30199: train_loss: 112.75204200711059
	 epoch: 0|iter: 30299: train_loss: 51.78630517241768
	 epoch: 0|iter: 30399: train_loss: 131.02025232520344
	 epoch: 0|iter: 30499: train_loss: 12.31159655364091
	 epoch: 0|iter: 30599: train_loss: 32.41489948275779
	 epoch: 0|iter: 30699: train_loss: 32.98297837733119
	 epoch: 0|iter: 30799: train_loss: 60.31153631644664
	 epoch: 0|iter: 30899: train_loss: 34.53590204387401
	 epoch: 0|iter: 30999: train_loss: 144.7019914503208
	 epoch: 0|iter: 31099: train_loss: 40.43213578500262
	 epoch: 0|iter: 31199: train_loss: 35.359389905073
	 epoch: 0|iter: 31299: train_loss: 28.25302423734731
	 epoch: 0|iter: 31399: train_loss: 326.77125870471264
	 epoch: 0|iter: 31499: train_loss: 38.37842604561497
	 epoch: 0|iter: 31599: train_loss: 134.682927035402
	 epoch: 0|iter: 31699: train_loss: 105.84493785737412
	 epoch: 0|iter: 31799: train_loss: 50.83282702402332
	 epoch: 0|iter: 31899: train_loss: 19.195645295259666
	 epoch: 0|iter: 31999: train_loss: 93.92304907512198
	 epoch: 0|iter: 32099: train_loss: 1340.527231320601
	 epoch: 0|iter: 32199: train_loss: 63.082595447432176
	 epoch: 0|iter: 32299: train_loss: 26.31358451658125
	 epoch: 0|iter: 32399: train_loss: 47.00867997760844
	 epoch: 0|iter: 32499: train_loss: 57.3011339598243
	 epoch: 0|iter: 32599: train_loss: 136.9601074950132
	 epoch: 0|iter: 32699: train_loss: 121.68633237870051
	 epoch: 0|iter: 32799: train_loss: 99.58377441899073
	 epoch: 0|iter: 32899: train_loss: 53.34841721211215
	 epoch: 0|iter: 32999: train_loss: 58.4614374552768
	 epoch: 0|iter: 33099: train_loss: 30.761031787235275
	 epoch: 0|iter: 33199: train_loss: 1050.0373947216192
	 epoch: 0|iter: 33299: train_loss: 94.06789642334152
	 epoch: 0|iter: 33399: train_loss: 38.59962444062218
	 epoch: 0|iter: 33499: train_loss: 118.63202751873456
	 epoch: 0|iter: 33599: train_loss: 36.52554932899824
	 epoch: 0|iter: 33699: train_loss: 41.68560798721318
	 epoch: 0|iter: 33799: train_loss: 80.27552800040873
	 epoch: 0|iter: 33899: train_loss: 26.22479676388061
	 epoch: 0|iter: 33999: train_loss: 199.33337093811085
	 epoch: 0|iter: 34099: train_loss: 103.54495672111548
	 epoch: 0|iter: 34199: train_loss: 98.64022227583823
	 epoch: 0|iter: 34299: train_loss: 46.4431881928557
	 epoch: 0|iter: 34399: train_loss: 857.897039979147
	 epoch: 0|iter: 34499: train_loss: 112.392166956115
	 epoch: 0|iter: 34599: train_loss: 79.10207093985154
	 epoch: 0|iter: 34699: train_loss: 127.10084229339095
	 epoch: 0|iter: 34799: train_loss: 72.46988313880618
	 epoch: 0|iter: 34899: train_loss: 49.51760435091664
	 epoch: 0|iter: 34999: train_loss: 41.10232294413834
	 epoch: 0|iter: 35099: train_loss: 380.5525411128796
	 epoch: 0|iter: 35199: train_loss: 1173.736972043379
	 epoch: 0|iter: 35299: train_loss: 75.36464328091988
	 epoch: 0|iter: 35399: train_loss: 46.66932165586422
	 epoch: 0|iter: 35499: train_loss: 53.58204276117802
	 epoch: 0|iter: 35599: train_loss: 46.71525424019349
	 epoch: 0|iter: 35699: train_loss: 16.347596290551902
	 epoch: 0|iter: 35799: train_loss: 77.0124774969345
	 epoch: 0|iter: 35899: train_loss: 76.60830885131705
	 epoch: 0|iter: 35999: train_loss: 49.56354805804544
	 epoch: 0|iter: 36099: train_loss: 349.00748535125365
	 epoch: 0|iter: 36199: train_loss: 26.228382101102998
	 epoch: 0|iter: 36299: train_loss: 125.61925118593673
	 epoch: 0|iter: 36399: train_loss: 208.08496223570233
	 epoch: 0|iter: 36499: train_loss: 49.37582503083721
	 epoch: 0|iter: 36599: train_loss: 64.8813565199653
	 epoch: 0|iter: 36699: train_loss: 29.667061953237308
	 epoch: 0|iter: 36799: train_loss: 551.4895055244845
	 epoch: 0|iter: 36899: train_loss: 105.99710928655612
	 epoch: 0|iter: 36999: train_loss: 131.88772436255036
	 epoch: 0|iter: 37099: train_loss: 58.5753524324576
	 epoch: 0|iter: 37199: train_loss: 28.07624292416434
	 epoch: 0|iter: 37299: train_loss: 78.01275455401294
	 epoch: 0|iter: 37399: train_loss: 35.395843006080696
	 epoch: 0|iter: 37499: train_loss: 21.39706131809026
	 epoch: 0|iter: 37599: train_loss: 74.31474727181188
	 epoch: 0|iter: 37699: train_loss: 48.67175761070204
	 epoch: 0|iter: 37799: train_loss: 79.34986061227714
	 epoch: 0|iter: 37899: train_loss: 36.15891759312235
	 epoch: 0|iter: 37999: train_loss: 189.4368114297717
	 epoch: 0|iter: 38099: train_loss: 40.18798942427301
	 epoch: 0|iter: 38199: train_loss: 54.349287039468
	 epoch: 0|iter: 38299: train_loss: 52.16512093465595
	 epoch: 0|iter: 38399: train_loss: 71.56483075391432
	 epoch: 0|iter: 38499: train_loss: 167.51672510419561
	 epoch: 0|iter: 38599: train_loss: 46.28044803149048
	 epoch: 0|iter: 38699: train_loss: 478.08159040777514
	 epoch: 0|iter: 38799: train_loss: 90.19114074054923
	 epoch: 0|iter: 38899: train_loss: 61.124551398683785
	 epoch: 0|iter: 38999: train_loss: 15.230850526563836
	 epoch: 0|iter: 39099: train_loss: 33.56906281218379
	 epoch: 0|iter: 39199: train_loss: 16.895609676465156
	 epoch: 0|iter: 39299: train_loss: 14.776187620954579
	 epoch: 0|iter: 39399: train_loss: 35.49948790723259
	 epoch: 0|iter: 39499: train_loss: 16.743814593079325
	 epoch: 0|iter: 39599: train_loss: 69.11736595777319
	 epoch: 0|iter: 39699: train_loss: 104.93519739272011
	 epoch: 0|iter: 39799: train_loss: 151.0435298016215
	 epoch: 0|iter: 39899: train_loss: 27.87793590456336
	 epoch: 0|iter: 39999: train_loss: 94.26553294712602
	 epoch: 0|iter: 40099: train_loss: 262.1533267528918
	 epoch: 0|iter: 40199: train_loss: 41.62179619426113
	 epoch: 0|iter: 40299: train_loss: 157.6646762339293
	 epoch: 0|iter: 40399: train_loss: 77.17860658660621
	 epoch: 0|iter: 40499: train_loss: 41.61247940816496
	 epoch: 0|iter: 40599: train_loss: 140.14232067968646
	 epoch: 0|iter: 40699: train_loss: 255.5472071128507
	 epoch: 0|iter: 40799: train_loss: 28.182066791944383
	 epoch: 0|iter: 40899: train_loss: 150.45203329795024
	 epoch: 0|iter: 40999: train_loss: 645.9167569771907
	 epoch: 0|iter: 41099: train_loss: 29.86596172009599
	 epoch: 0|iter: 41199: train_loss: 1095.3538203501837
	 epoch: 0|iter: 41299: train_loss: 52.49367582768464
	 epoch: 0|iter: 41399: train_loss: 65.6494576789202
	 epoch: 0|iter: 41499: train_loss: 44.58024467698555
	 epoch: 0|iter: 41599: train_loss: 74.90949196495008
	 epoch: 0|iter: 41699: train_loss: 112.49353271136236
	 epoch: 0|iter: 41799: train_loss: 88.64109447862566
	 epoch: 0|iter: 41899: train_loss: 16.285193477001354
	 epoch: 0|iter: 41999: train_loss: 81.25285439302893
	 epoch: 0|iter: 42099: train_loss: 66.69045019926001
	 epoch: 0|iter: 42199: train_loss: 74.21019490845055
	 epoch: 0|iter: 42299: train_loss: 272.8638873948129
	 epoch: 0|iter: 42399: train_loss: 12.937125982959792
	 epoch: 0|iter: 42499: train_loss: 22.096640662686994
	 epoch: 0|iter: 42599: train_loss: 81.04865479833732
	 epoch: 0|iter: 42699: train_loss: 132.79098551243982
	 epoch: 0|iter: 42799: train_loss: 83.67323698584434
	 epoch: 0|iter: 42899: train_loss: 680.2576549728487
	 epoch: 0|iter: 42999: train_loss: 24.216160233149612
	 epoch: 0|iter: 43099: train_loss: 196.63928232708662
	 epoch: 0|iter: 43199: train_loss: 234.3231056094642
	 epoch: 0|iter: 43299: train_loss: 110.93773173891782
	 epoch: 0|iter: 43399: train_loss: 173.2014464132723
	 epoch: 0|iter: 43499: train_loss: 50.57128231239488
	 epoch: 0|iter: 43599: train_loss: 94.35051061062055
	 epoch: 0|iter: 43699: train_loss: 27.848697766776148
	 epoch: 0|iter: 43799: train_loss: 28.86433497411148
	 epoch: 0|iter: 43899: train_loss: 1387.896618072752
	 epoch: 0|iter: 43999: train_loss: 29.523481582436595
	 epoch: 0|iter: 44099: train_loss: 71.40357920867224
	 epoch: 0|iter: 44199: train_loss: 36.900876635106684
	 epoch: 0|iter: 44299: train_loss: 90.26316646601074
	 epoch: 0|iter: 44399: train_loss: 8.369062470238564
	 epoch: 0|iter: 44499: train_loss: 33.71871798623071
	 epoch: 0|iter: 44599: train_loss: 68.64796229381166
	 epoch: 0|iter: 44699: train_loss: 71.42501042529
	 epoch: 0|iter: 44799: train_loss: 36.708527927535826
	 epoch: 0|iter: 44899: train_loss: 40.241753851293645
	 epoch: 0|iter: 44999: train_loss: 49.89350765211944
	 epoch: 0|iter: 45099: train_loss: 60.06689094961004
	 epoch: 0|iter: 45199: train_loss: 190.0529038498647
	 epoch: 0|iter: 45299: train_loss: 44.47868440501175
	 epoch: 0|iter: 45399: train_loss: 133.90575221720002
	 epoch: 0|iter: 45499: train_loss: 62.88836294042663
	 epoch: 0|iter: 45599: train_loss: 61.96097524484753
	 epoch: 0|iter: 45699: train_loss: 36.30725230495129
	 epoch: 0|iter: 45799: train_loss: 60.8274509290508
	 epoch: 0|iter: 45899: train_loss: 20.640867780249472
	 epoch: 0|iter: 45999: train_loss: 721.1413948261494
	 epoch: 0|iter: 46099: train_loss: 38.20369893530007
	 epoch: 0|iter: 46199: train_loss: 36.76701628778886
	 epoch: 0|iter: 46299: train_loss: 38.20080397178336
	 epoch: 0|iter: 46399: train_loss: 48.93229269246708
	 epoch: 0|iter: 46499: train_loss: 90.77178992516342
	 epoch: 0|iter: 46599: train_loss: 57.0479721962551
	 epoch: 0|iter: 46699: train_loss: 104.57137625759685
	 epoch: 0|iter: 46799: train_loss: 202.25282836671968
	 epoch: 0|iter: 46899: train_loss: 45.80100631495205
	 epoch: 0|iter: 46999: train_loss: 49.55553520247808
	 epoch: 0|iter: 47099: train_loss: 172.90715268138484
	 epoch: 0|iter: 47199: train_loss: 96.79919549473887
	 epoch: 0|iter: 47299: train_loss: 8.449934251066887
	 epoch: 0|iter: 47399: train_loss: 463.8359909940173
	 epoch: 0|iter: 47499: train_loss: 86.35406049100212
	 epoch: 0|iter: 47599: train_loss: 59.54377716042907
	 epoch: 0|iter: 47699: train_loss: 53.501316856640166
	 epoch: 0|iter: 47799: train_loss: 19.7026713793602
	 epoch: 0|iter: 47899: train_loss: 241.3895292223275
	 epoch: 0|iter: 47999: train_loss: 66.76016445120973
	 epoch: 0|iter: 48099: train_loss: 12.987398607560669
	 epoch: 0|iter: 48199: train_loss: 38.04072055017579
	 epoch: 0|iter: 48299: train_loss: 122.04356296644796
	 epoch: 0|iter: 48399: train_loss: 83.23125243000997
	 epoch: 0|iter: 48499: train_loss: 27.130095888322455
	 epoch: 0|iter: 48599: train_loss: 68.84688004268205
	 epoch: 0|iter: 48699: train_loss: 206.52298006264806
	 epoch: 0|iter: 48799: train_loss: 106.61410880881306
	 epoch: 0|iter: 48899: train_loss: 148.38963179158358
	 epoch: 0|iter: 48999: train_loss: 109.6707605774394
	 epoch: 0|iter: 49099: train_loss: 29.942791901591
	 epoch: 0|iter: 49199: train_loss: 150.4567892095367
	 epoch: 0|iter: 49299: train_loss: 73.84179208820952
	 epoch: 0|iter: 49399: train_loss: 104.57398828559641
	 epoch: 0|iter: 49499: train_loss: 83.43148862042457
	 epoch: 0|iter: 49599: train_loss: 100.394308558564
	 epoch: 0|iter: 49699: train_loss: 71.80139301810664
	 epoch: 0|iter: 49799: train_loss: 46.97148459250468
	 epoch: 0|iter: 49899: train_loss: 111.79031569237249
	 epoch: 0|iter: 49999: train_loss: 871.7587437643163
	 epoch: 0|iter: 50099: train_loss: 200.4801803032226
	 epoch: 0|iter: 50199: train_loss: 22.82140077904483
	 epoch: 0|iter: 50299: train_loss: 100.98276809139547
	 epoch: 0|iter: 50399: train_loss: 18.67377516216421
	 epoch: 0|iter: 50499: train_loss: 31.752395011581942
	 epoch: 0|iter: 50599: train_loss: 21.07571491031172
	 epoch: 0|iter: 50699: train_loss: 14.732362689481873
	 epoch: 0|iter: 50799: train_loss: 121.91285117702868
	 epoch: 0|iter: 50899: train_loss: 39.320918191220485
	 epoch: 0|iter: 50999: train_loss: 76.72915206776563
	 epoch: 0|iter: 51099: train_loss: 31.169192653188347
	 epoch: 0|iter: 51199: train_loss: 40.671214962394124
	 epoch: 0|iter: 51299: train_loss: 289.53702483587415
	 epoch: 0|iter: 51399: train_loss: 44.11260657713327
	 epoch: 0|iter: 51499: train_loss: 42.206644135015814
	 epoch: 0|iter: 51599: train_loss: 43.12312199617662
	 epoch: 0|iter: 51699: train_loss: 99.92269576399076
	 epoch: 0|iter: 51799: train_loss: 35.6671469438924
	 epoch: 0|iter: 51899: train_loss: 35.94337236947334
	 epoch: 0|iter: 51999: train_loss: 83.46849942790486
	 epoch: 0|iter: 52099: train_loss: 94.19334859510325
	 epoch: 0|iter: 52199: train_loss: 162.90306294743323
	 epoch: 0|iter: 52299: train_loss: 42.22894149408632
	 epoch: 0|iter: 52399: train_loss: 27.241955469171227
	 epoch: 0|iter: 52499: train_loss: 13.275448816035835
	 epoch: 0|iter: 52599: train_loss: 60.02865475913223
	 epoch: 0|iter: 52699: train_loss: 73.5386588441445
	 epoch: 0|iter: 52799: train_loss: 13.290310558233076
	 epoch: 0|iter: 52899: train_loss: 96.19001912533251
	 epoch: 0|iter: 52999: train_loss: 4512.971283223634
	 epoch: 0|iter: 53099: train_loss: 40.25103557776715
	 epoch: 0|iter: 53199: train_loss: 47.59753479411001
	 epoch: 0|iter: 53299: train_loss: 22.41706443473691
	 epoch: 0|iter: 53399: train_loss: 125.48495997426468
	 epoch: 0|iter: 53499: train_loss: 50.51785005246038
	 epoch: 0|iter: 53599: train_loss: 56.431962576001546
	 epoch: 0|iter: 53699: train_loss: 79.4099915821844
	 epoch: 0|iter: 53799: train_loss: 17.149317673569804
	 epoch: 0|iter: 53899: train_loss: 16.15894861554633
	 epoch: 0|iter: 53999: train_loss: 41.110546663109346
	 epoch: 0|iter: 54099: train_loss: 44.94321074510406
	 epoch: 0|iter: 54199: train_loss: 20.148178176930198
	 epoch: 0|iter: 54299: train_loss: 186.20114566237245
	 epoch: 0|iter: 54399: train_loss: 52.43902330080187
	 epoch: 0|iter: 54499: train_loss: 51.74746630243544
	 epoch: 0|iter: 54599: train_loss: 40.82082216714799
	 epoch: 0|iter: 54699: train_loss: 27.36792071508144
	 epoch: 0|iter: 54799: train_loss: 84.72534677612262
	 epoch: 0|iter: 54899: train_loss: 46.371471960303396
	 epoch: 0|iter: 54999: train_loss: 33.06572146611913
	 epoch: 0|iter: 55099: train_loss: 31.44890611097657
	 epoch: 0|iter: 55199: train_loss: 139.98080465627396
	 epoch: 0|iter: 55299: train_loss: 61.30772846845824
	 epoch: 0|iter: 55399: train_loss: 72.3068936764978
	 epoch: 0|iter: 55499: train_loss: 556.8356988316439
	 epoch: 0|iter: 55599: train_loss: 98.07013250674397
	 epoch: 0|iter: 55699: train_loss: 31.474470438876818
	 epoch: 0|iter: 55799: train_loss: 1450.1487319853873
	 epoch: 0|iter: 55899: train_loss: 309.5724781700575
	 epoch: 0|iter: 55999: train_loss: 122.88788771957607
	 epoch: 0|iter: 56099: train_loss: 16.972781439114748
	 epoch: 0|iter: 56199: train_loss: 26.325572425730314
	 epoch: 0|iter: 56299: train_loss: 82.70248140149718
	 epoch: 0|iter: 56399: train_loss: 57.05700794835132
	 epoch: 0|iter: 56499: train_loss: 32.30437029334289
	 epoch: 0|iter: 56599: train_loss: 27.289690164372306
	 epoch: 0|iter: 56699: train_loss: 44.359145618205204
	 epoch: 0|iter: 56799: train_loss: 76.28171498834874
	 epoch: 0|iter: 56899: train_loss: 16.384534488121886
	 epoch: 0|iter: 56999: train_loss: 36.93175694089751
	 epoch: 0|iter: 57099: train_loss: 109.10882709772687
	 epoch: 0|iter: 57199: train_loss: 27.0966911666032
	 epoch: 0|iter: 57299: train_loss: 40.27019639260577
	 epoch: 0|iter: 57399: train_loss: 31.36228196732251
	 epoch: 0|iter: 57499: train_loss: 159.79984963228767
	 epoch: 0|iter: 57599: train_loss: 127.91901357242557
	 epoch: 0|iter: 57699: train_loss: 67.41207268874837
	 epoch: 0|iter: 57799: train_loss: 51.640758791663075
	 epoch: 0|iter: 57899: train_loss: 34.34530172137754
	 epoch: 0|iter: 57999: train_loss: 838.4478618181599
	 epoch: 0|iter: 58099: train_loss: 57.07904737030294
	 epoch: 0|iter: 58199: train_loss: 19.80057734810705
	 epoch: 0|iter: 58299: train_loss: 17.439425089056105
	 epoch: 0|iter: 58399: train_loss: 10.190546797865704
	 epoch: 0|iter: 58499: train_loss: 46.009200336856075
	 epoch: 0|iter: 58599: train_loss: 95.39486545236551
	 epoch: 0|iter: 58699: train_loss: 270.1996928556725
	 epoch: 0|iter: 58799: train_loss: 52.80647031463336
	 epoch: 0|iter: 58899: train_loss: 21.063230425789484
	 epoch: 0|iter: 58999: train_loss: 161.57977994047405
	 epoch: 0|iter: 59099: train_loss: 28.34290426660346
	 epoch: 0|iter: 59199: train_loss: 23.98395538356079
	 epoch: 0|iter: 59299: train_loss: 431.26745710885643
	 epoch: 0|iter: 59399: train_loss: 57.42491576534687
	 epoch: 0|iter: 59499: train_loss: 116.11800041354714
	 epoch: 0|iter: 59599: train_loss: 335.6274698333504
	 epoch: 0|iter: 59699: train_loss: 33.78896126934283
	 epoch: 0|iter: 59799: train_loss: 277.77785867610754
	 epoch: 0|iter: 59899: train_loss: 13.734305534997885
	 epoch: 0|iter: 59999: train_loss: 101.01536930499253
	 epoch: 0|iter: 60099: train_loss: 114.14765020027518
	 epoch: 0|iter: 60199: train_loss: 24.62348790693983
	 epoch: 0|iter: 60299: train_loss: 43.53233698481067
	 epoch: 0|iter: 60399: train_loss: 64.97846169035057
	 epoch: 0|iter: 60499: train_loss: 1126.0228668306636
	 epoch: 0|iter: 60599: train_loss: 27.968482869248096
	 epoch: 0|iter: 60699: train_loss: 41.16108123413993
	 epoch: 0|iter: 60799: train_loss: 177.74671803681073
	 epoch: 0|iter: 60899: train_loss: 33.52555641277483
	 epoch: 0|iter: 60999: train_loss: 18.137807356057525
	 epoch: 0|iter: 61099: train_loss: 46.027639579424395
	 epoch: 0|iter: 61199: train_loss: 30.590118746577694
	 epoch: 0|iter: 61299: train_loss: 17.89593344313171
epoch: 288.45989990234375
Traceback (most recent call last):
  File "/home/wwh/graphcast/main.py", line 16, in <module>
    trainer.train()
  File "/home/wwh/graphcast/graphcast.py", line 68, in train
    self.valid()
  File "/home/wwh/graphcast/graphcast.py", line 196, in valid
    predict = self.model(input)
  File "/home/wwh/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wwh/graphcast/model/graphcastnet.py", line 77, in forward
    vg, vm, em, _, em2g = self.encoder(
  File "/home/wwh/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wwh/graphcast/model/layer.py", line 196, in forward
    vg, vm, eg2m = self.g2m_gnn(vg, vm, eg2m) 
  File "/home/wwh/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wwh/graphcast/model/layer.py", line 146, in forward
    mesh_node_aggr, g2m_edge_attr = self.interaction((grid_node_feats, mesh_node_feats, g2m_edge_feats))
  File "/home/wwh/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wwh/graphcast/model/layer.py", line 66, in forward
    update_edge_feats = self.edge_fn(torch.concat((src_feats, dst_feats, edge_feats), axis=-1))
  File "/home/wwh/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wwh/graphcast/model/layer.py", line 20, in forward
    return self.network(x)     
  File "/home/wwh/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wwh/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/wwh/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wwh/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/activation.py", line 396, in forward
    return F.silu(input, inplace=self.inplace)
  File "/home/wwh/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py", line 2059, in silu
    return torch._C._nn.silu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 592.00 MiB (GPU 0; 79.15 GiB total capacity; 72.46 GiB already allocated; 188.62 MiB free; 77.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
